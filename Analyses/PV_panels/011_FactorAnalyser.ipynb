{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from scipy.misc import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images and score masks successfully read.\n"
     ]
    }
   ],
   "source": [
    "def get_name(filepath):\n",
    "    return filepath.split('/')[-1]\n",
    "\n",
    "n_cracked   = len(glob('./Data/170729_Panel_Solos_Resized/CrackedScores/*.png'))\n",
    "n_query     = 3\n",
    "\n",
    "cracked_score_fps   = glob('./Data/170729_Panel_Solos_Resized/CrackedScores/*.png')\n",
    "uncracked_score_fps = glob('./Data/170729_Panel_Solos_Resized/UncrackedScores/*.png') \n",
    "\n",
    "image_filepaths     = (['./Data/170729_Panel_Solos_Resized/Cracked/' + get_name(fp) \n",
    "                         for fp in cracked_score_fps] +\n",
    "                       ['./Data/170729_Panel_Solos_Resized/Uncracked/' + get_name(fp)\n",
    "                         for fp in uncracked_score_fps])\n",
    "\n",
    "score_filepaths  = ( cracked_score_fps +\n",
    "                      uncracked_score_fps )\n",
    "\n",
    "scores  = np.expand_dims(np.array(\n",
    "                [imread(fp) for fp in score_filepaths], dtype = 'float32'), axis = -1) / 255\n",
    "scores  = np.concatenate([scores, 1 - scores], axis = -1) # One-hot encode scores\n",
    "images  = np.expand_dims(np.array(\n",
    "                [imread(fp) for fp in image_filepaths], dtype = 'float32'), axis = -1)\n",
    "\n",
    "query_ix     = np.random.randint(0, n_cracked, 3)\n",
    "query_mask   = np.zeros([scores.shape[0]], dtype = bool)\n",
    "query_mask[query_ix] = True\n",
    "\n",
    "query_scores = scores[query_mask, :, :, :]\n",
    "query_images = images[query_mask, :, :, :]\n",
    "scores       = scores[np.logical_not(query_mask), :, :, :]\n",
    "images       = images[np.logical_not(query_mask), :, :, :]\n",
    "\n",
    "labels       = (np.sum(scores, axis = (1, 2)) > 0).squeeze()[:, 0]\n",
    "\n",
    "print('Images and score masks successfully read.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that we are receiving a batch of 40x40 pixel patches\n",
    "\n",
    "patch_size  = 40\n",
    "input_size  = patch_size**2\n",
    "batch_size  = 32\n",
    "n_factors   = 2\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('Inputs'):\n",
    "        X      = tf.placeholder(tf.float32, [batch_size, input_size])\n",
    "    \n",
    "    with tf.name_scope('Variables'):\n",
    "        mu    = tf.Variable(tf.truncated_normal(\n",
    "                                shape = [input_size]))\n",
    "        Sigma = tf.Variable(tf.eye(input_size))\n",
    "        Phi   = tf.Variable(tf.truncated_normal(\n",
    "                                shape = [input_size, n_factors]))\n",
    "        \n",
    "    def E_step(X, Phi, Sigma, mu):\n",
    "        \"\"\"\n",
    "            Returns a [batch_size, n_factors] matrix.\n",
    "        \n",
    "            X     : [batch_size, input_size] matrix of observations.\n",
    "            Phi   : [input_size, n_factors] matrix of factors.\n",
    "            Sigma : [input_size, input_size] diagonal covariance matrix.\n",
    "            mu    : [input_size] vector located at x's mean.\n",
    "        \"\"\"\n",
    "        inv_Sigma = tf.diag(tf.divide(1, tf.diag_part(Sigma))) # [input_size, input_size]\n",
    "        tmp1      = tf.tensordot(tf.transpose(Phi), inv_Sigma, axes = [[1], [0]]) # [n_factors, input_size]\n",
    "        tmp2      = tf.tensordot(tmp1, Phi, axes = [[1], [0]]) # [n_factors, n_factors]\n",
    "        tmp3      = tf.matrix_inverse(tmp2 + tf.eye(n_factors)) #[n_factors, n_factors]\n",
    "        tmp4      = tf.tensordot(tmp3, tf.tensordot(tf.transpose(Phi), inv_Sigma, axes = [[1], [0]]),\n",
    "                                 axes = [[1], [0]]) # [n_factors, input_size]\n",
    "        Eh        = tf.tensordot(X - mu, tf.transpose(tmp4), axes = [[1], [0]]) # [batch_size, n_factors]\n",
    "        outer     = [None]*batch_size\n",
    "        for i in range(batch_size):\n",
    "            outer[i] = tmp3 + tf.tensordot(tf.slice(Eh, [i, 0], [1, -1]),\n",
    "                                           tf.slice(Eh, [i, 0], [1, -1]),\n",
    "                                           axes = [[1], [1]])\n",
    "        Ehh       = tf.stack(outer, axis = 0)\n",
    "        return Eh, Ehh\n",
    "    \n",
    "    def M_step(X, mu, Phi, Eh, Ehh):\n",
    "        \"\"\"\n",
    "            Returns ops to update the model's parameters.\n",
    "            Updates are performed analytically.\n",
    "            \n",
    "            mu    : [input_size]\n",
    "            Sigma : [input_size, input_size]\n",
    "            Phi   : [input_size, n_factors]\n",
    "        \"\"\"\n",
    "        mu      = update_mu(X)\n",
    "        Phi     = update_Phi(X, mu, Eh, Ehh)\n",
    "        Sigma   = update_Sigma(X, mu, Phi, Eh)\n",
    "        return mu, Phi, Sigma\n",
    "    \n",
    "    def update_mu(X):\n",
    "        \"\"\"\n",
    "            Returns an op that updates mean.\n",
    "            \n",
    "            X          : [batch_size, input_size]\n",
    "            mu_hat     : [input_size]\n",
    "            batch_size : scalar\n",
    "        \"\"\"\n",
    "        mu_hat = tf.reduce_mean(X, axis = 0)\n",
    "        return mu_hat\n",
    "    \n",
    "    def update_Phi(X, mu, Eh, Ehh):\n",
    "        \"\"\"\n",
    "            Returns an op that updates matrix of factors.\n",
    "            \n",
    "            X          : [batch_size, input_size]\n",
    "            mu         : [input_size]\n",
    "            batch_size : scalar\n",
    "            Eh         : [batch_size, n_factors]\n",
    "            Ehh        : [batch_size, n_factors, n_factors]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute outer products over all expectation/example pairs\n",
    "        outer = [None]*batch_size\n",
    "        for i in range(batch_size):\n",
    "            outer[i] = tf.tensordot(tf.transpose(tf.slice(X, begin = [i, 0], size = [1, -1])), # [n_factors, 1]\n",
    "                                    tf.slice(Eh, begin = [i, 0], size = [1, -1]),              # [input_size, 1]\n",
    "                                    axes = [[1], [0]])                                         # [input_size, n_factors]\n",
    "            \n",
    "        tmp3     = tf.stack(outer, axis = 0)\n",
    "        tmp2     = tf.matrix_inverse(tf.reduce_sum(Ehh, axis = 0)) # [n_factors, n_factors]\n",
    "        Phi_hat  = tf.tensordot(tf.reduce_sum(tmp3, axis = 0), tmp2, axes = [[1], [0]])\n",
    "        return Phi_hat\n",
    "    \n",
    "    def update_Sigma(X, mu, Phi, Eh):\n",
    "        \"\"\"\n",
    "            Returns an op that updates the diagonal covariance matrix.\n",
    "            \n",
    "            X          : [batch_size, input_size]\n",
    "            mu         : [input_size]\n",
    "            batch_size : scalar\n",
    "            Eh         : [batch_size, n_factors]\n",
    "            Phi        : [input_size, n_factors]\n",
    "            Sigma_hat  : [input_size, input_size]\n",
    "        \"\"\"\n",
    "        squares = [None]*batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            xi          = tf.slice(X, begin = [i, 0], size = [1, -1])\n",
    "            Ehi         = tf.transpose(tf.slice(Eh, begin = [i, 0], size = [1, -1]))\n",
    "            deviation   = tf.transpose(xi - mu) # [input_size, 1]\n",
    "            tmp1        = tf.pow(deviation, 2) # [input_size, 1]\n",
    "            tmp2        = tf.tensordot(Phi, Ehi, axes = [[1], [0]]) # [input_size, 1]\n",
    "            squares[i]  = tf.transpose(tmp1 - tf.multiply(tmp2, deviation)) # [input_size, 1]\n",
    "\n",
    "        var_diag  = tf.reduce_mean(tf.stack(squares, axis = 0), axis = 0)\n",
    "        Sigma_hat = tf.diag(tf.squeeze(var_diag))\n",
    "        return Sigma_hat\n",
    "        \n",
    "    with tf.name_scope('Optimization'):\n",
    "        with tf.name_scope('E_step'):\n",
    "            Eh, Ehh   = E_step(X, Phi, Sigma, mu) # [batch_size, n_factors] - Each row corresponds to a single example\n",
    "            \n",
    "        with tf.name_scope('M_step'):\n",
    "            with tf.control_dependencies([Eh, Ehh]):\n",
    "                mu, Phi, Sigma = M_step(X, mu, Phi, Eh, Ehh)\n",
    "        \n",
    "        optimize       = tf.group(mu, Phi, Sigma)\n",
    "        \n",
    "    with tf.name_scope('Inference'):\n",
    "        covar         = tf.tensordot(Phi, tf.transpose(Phi), axes = [[1], [0]]) + Sigma\n",
    "        mvn           = tf.contrib.distributions.MultivariateNormalFullCovariance(loc = mu,\n",
    "                                                                                  covariance_matrix = covar)\n",
    "        probabilities = mvn.prob(X) # [batch_size] vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minibatch:\n",
    "    def __init__(self, minibatch_size, patch_size):\n",
    "        self.candidate_ix   = np.array(np.nonzero(scores[:,:,:,0] > 0.5)).T\n",
    "        self.patch_size     = patch_size\n",
    "        self.minibatch_size = minibatch_size\n",
    "    \n",
    "    def get(self):\n",
    "        rand_ints    = np.random.randint(0, self.candidate_ix.shape[0],self.minibatch_size)\n",
    "        chosen_ix    = self.candidate_ix[rand_ints, :]\n",
    "        patches      = self._get_patches(images, chosen_ix)\n",
    "        flat_patches = np.reshape(patches, newshape = [self.minibatch_size, -1])\n",
    "        return flat_patches\n",
    "    \n",
    "    def _get_patches(self, images, ix):\n",
    "        \"\"\"\n",
    "            Accepts a stack of images [N, R, Co, Ch], indices to sample at, patch sizes.\n",
    "            \n",
    "            Returns a stack of image patches\n",
    "        \"\"\"\n",
    "        ps         = self.patch_size\n",
    "        patches    = np.zeros([ix.shape[0], patch_size, patch_size, images.shape[3]])\n",
    "\n",
    "        for j, i in enumerate(ix):\n",
    "            if (j == 0) or np.any(images[i[0], :, :, :] != img):\n",
    "                img  = images[i[0], :, :, :]\n",
    "                pimg = np.pad(img, pad_width = [[ps, ps], [ps, ps], [0, 0]],\n",
    "                              mode = 'reflect')\n",
    "            patches[j, :, :, :] =  pimg[i[1] + ps // 2 : i[1] + (3 * ps // 2),\n",
    "                                        i[2] + ps // 2 : i[2] + (3 * ps // 2), :]\n",
    "\n",
    "        return patches\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized.\n",
      "\n",
      "Step  0 : Phi = [[-1513351.625  1513353.375]\n",
      " [-1508007.     1508008.875]\n",
      " [-1493993.375  1493995.125]\n",
      " ..., \n",
      " [-1627272.5    1627273.875]\n",
      " [-1675145.375  1675146.875]\n",
      " [-1674264.     1674265.5  ]]\n",
      "Step  1 : Phi = [[-1513351.625  1513353.375]\n",
      " [-1508007.     1508008.875]\n",
      " [-1493993.375  1493995.125]\n",
      " ..., \n",
      " [-1627272.5    1627273.875]\n",
      " [-1675145.375  1675146.875]\n",
      " [-1674264.     1674265.5  ]]\n",
      "Step  2 : Phi = [[-1513351.625  1513353.375]\n",
      " [-1508007.     1508008.875]\n",
      " [-1493993.375  1493995.125]\n",
      " ..., \n",
      " [-1627272.5    1627273.875]\n",
      " [-1675145.375  1675146.875]\n",
      " [-1674264.     1674265.5  ]]\n",
      "Step  3 : Phi = [[-1513351.625  1513353.375]\n",
      " [-1508007.     1508008.875]\n",
      " [-1493993.375  1493995.125]\n",
      " ..., \n",
      " [-1627272.5    1627273.875]\n",
      " [-1675145.375  1675146.875]\n",
      " [-1674264.     1674265.5  ]]\n",
      "Step  4 : Phi = [[-1513351.625  1513353.375]\n",
      " [-1508007.     1508008.875]\n",
      " [-1493993.375  1493995.125]\n",
      " ..., \n",
      " [-1627272.5    1627273.875]\n",
      " [-1675145.375  1675146.875]\n",
      " [-1674264.     1674265.5  ]]\n",
      "Step  5 : Phi = [[-1513351.625  1513353.375]\n",
      " [-1508007.     1508008.875]\n",
      " [-1493993.375  1493995.125]\n",
      " ..., \n",
      " [-1627272.5    1627273.875]\n",
      " [-1675145.375  1675146.875]\n",
      " [-1674264.     1674265.5  ]]\n",
      "Step  6 : Phi = [[-1513351.625  1513353.375]\n",
      " [-1508007.     1508008.875]\n",
      " [-1493993.375  1493995.125]\n",
      " ..., \n",
      " [-1627272.5    1627273.875]\n",
      " [-1675145.375  1675146.875]\n",
      " [-1674264.     1674265.5  ]]\n",
      "Step  7 : Phi = [[-1513351.625  1513353.375]\n",
      " [-1508007.     1508008.875]\n",
      " [-1493993.375  1493995.125]\n",
      " ..., \n",
      " [-1627272.5    1627273.875]\n",
      " [-1675145.375  1675146.875]\n",
      " [-1674264.     1674265.5  ]]\n",
      "Step  8 : Phi = [[-1513351.625  1513353.375]\n",
      " [-1508007.     1508008.875]\n",
      " [-1493993.375  1493995.125]\n",
      " ..., \n",
      " [-1627272.5    1627273.875]\n",
      " [-1675145.375  1675146.875]\n",
      " [-1674264.     1674265.5  ]]\n",
      "Step  9 : Phi = [[-1513351.625  1513353.375]\n",
      " [-1508007.     1508008.875]\n",
      " [-1493993.375  1493995.125]\n",
      " ..., \n",
      " [-1627272.5    1627273.875]\n",
      " [-1675145.375  1675146.875]\n",
      " [-1674264.     1674265.5  ]]\n"
     ]
    }
   ],
   "source": [
    "mb = Minibatch(batch_size, patch_size)\n",
    "training_steps = 10\n",
    "\n",
    "bat = mb.get()\n",
    "\n",
    "with tf.Session(graph = graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    print('Model initialized.\\n')\n",
    "    \n",
    "    for i in range(training_steps):\n",
    "        fd = { X :  bat}\n",
    "        _, phi  = session.run([optimize, Phi], feed_dict = fd)\n",
    "\n",
    "        if i % 1 == 0:\n",
    "            print('Step {:^3d}: Phi = {}'.format(i, phi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHY ISNT IT WORKING"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
