{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves to explore various methods of detecting textures in images. The emphasis here is on finding representations that separate a texture from its background: The classifier used is included simply to provide a measure of representation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image spec.:\n",
    "    - Do not include axes.\n",
    "    - (15, 5) plots where necessary (title in Illustrator).\n",
    "    - Grayscale single-channel images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this document, texture can be characterised as a distinct configuration of the following factors:\n",
    "    - Intensity\n",
    "    - Periodicity\n",
    "    - Non-periodic variation (i.e. randomness)\n",
    "    - Orientation\n",
    "    - Direction\n",
    "    - Size\n",
    "\n",
    "Additional information about a texture may also be derived from:\n",
    "    - Position relative to the camera.\n",
    "    - Orientation relative to the camera.\n",
    "    - The texture's background.\n",
    "    - The transition between texture and background.\n",
    "    \n",
    "Before examining specific filtering methods, we will briefly list what methods are available for texture analysis, along with their motivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Peak-counting*\n",
    "    - Suppress local non-maximum intensities.\n",
    "    - Apply a threshold.\n",
    "    - Count the peaks in the vicinity.\n",
    "    \n",
    "*Autocorrelation*\n",
    "    - Compute correlations between pixel intensities across some period at a given orientation (e.g. every other pixel, every 5 pixels etc.).\n",
    "    \n",
    "*Fourier analysis*\n",
    "    - Map the image from the intensity domain to the spatial frequency domain\n",
    "    \n",
    "*Co-occurence matrices, with summary statistics*\n",
    "    - Compute the relative frequency with which two distinct intensities reccur at a given orientation and radial distance.\n",
    "    - Summary statistics - energy, entropy, inertia, correlation, local homogeneity.\n",
    "    \n",
    "*Texture energy via Laws' filters*\n",
    "    - Begin with three kernels designed to extract edges, spots, and local intensity.\n",
    "    - Take these kernels' outer products to derive a set of masks.\n",
    "    - Convolve these masks with the images, then smooth (why smooth?).\n",
    "    - Square the resulting smoothed values to obtain energy measures.\n",
    "    - Combine the various energy measurements to produce features useful for classification.\n",
    "    \n",
    "*Texture energy via eigenfilters*\n",
    "    - Compute the covariance matrix over a texture region.\n",
    "    - Obtain the eigenvectors of this covariance matrix.\n",
    "    - Use these eigenvectors as kernels with which to extract energy measures.\n",
    "    \n",
    "*Local rank correlation*\n",
    "\n",
    "*Forced-choice method*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from skimage.exposure import equalize_adapthist\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display the image we'll be working with\n",
    "from scipy.misc import imread\n",
    "\n",
    "img_dir = \"./Data/170812_RustAnalysis/\"\n",
    "sav_dir = \"./Results/012_TextureAnalysis/\"\n",
    "raw_image = imread(img_dir + '001.png')[:, :, :-1]\n",
    "raw_mask  = imread(img_dir + '006.png')\n",
    "raw_mask  = np.logical_and(raw_mask[:,:,0] < 20, raw_mask[:, :, 1] > 240)\n",
    "raw_mask  = (resize(raw_mask, raw_image.shape, mode = 'reflect') > 0.1)[:,:,0].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "plt.imshow(raw_image)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "plt.imshow(raw_mask)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalize image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "equalized_image = equalize_adapthist(raw_image, 101)\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.imshow(equalized_image)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display a patch of the rust\n",
    "rust_element = equalized_image[130:160, 260:-150]\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.imshow(rust_element)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this image I perceive multiple levels of variation. There are variations in colour with periods of between 2 and 40 pixels, which inclines me to think that a 40 x 40 region should be sufficient for our purposes. We could use another size, but this is fine for our purposes at present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rust_subelement = rust_element[10:30, 20:40]\n",
    "plt.figure(figsize = (6, 6))\n",
    "plt.imshow(rust_subelement)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class convolutional_softmax_classifier:\n",
    "    \n",
    "    def __init__(self, kernel_size, in_channels):\n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            with tf.name_scope('Inputs'):\n",
    "                self.input_image = tf.placeholder(tf.float32, [None, None, None, in_channels]) # n-channel\n",
    "                self.input_mask  = tf.placeholder(tf.float32, [None, None, None]) # Single-channel\n",
    "\n",
    "            with tf.name_scope('Variables'):\n",
    "                self.weights     = tf.Variable(tf.truncated_normal(shape  = [kernel_size, kernel_size, in_channels, 2],\n",
    "                                                                  mean   = 0.,\n",
    "                                                                  stddev = 1.))\n",
    "                self.biases      = tf.Variable(tf.zeros(shape = [1, 1, 1, 2]))\n",
    "\n",
    "            with tf.name_scope('Model'):\n",
    "                with tf.name_scope('Standardization'):\n",
    "                    channel_mean       = tf.expand_dims(tf.reduce_mean(self.input_image, axis = -1), axis = -1)\n",
    "                    channel_var        = tf.reduce_mean(tf.subtract(self.input_image, channel_mean)**2)\n",
    "                    standardized_input = (self.input_image - channel_mean)/tf.sqrt(channel_var)\n",
    "                    standardized_mask  = self.input_mask/tf.reduce_max(self.input_mask)\n",
    "\n",
    "                with tf.name_scope('LinearProjection'):\n",
    "                    logits      = tf.nn.conv2d(standardized_input, self.weights,\n",
    "                                               strides = [1, 1, 1, 1], padding = 'SAME') + self.biases\n",
    "                    self.predictions = tf.nn.softmax(logits, dim = -1)\n",
    "\n",
    "                with tf.name_scope('Loss'):\n",
    "                    ohe_logits  = tf.transpose(tf.reshape(logits, shape = [2, -1]))\n",
    "                    ohe_mask    = tf.reshape(standardized_mask, shape = [-1])\n",
    "                    ohe_labels  = tf.stack([ohe_mask, 1 - ohe_mask], axis = -1)\n",
    "                    self.loss   = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = ohe_labels,\n",
    "                                                                                         logits = ohe_logits))\n",
    "\n",
    "            with tf.name_scope('Optimization'):\n",
    "                self.optimize    = tf.train.GradientDescentOptimizer(0.05).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ksize = 5\n",
    "training_steps = 5001\n",
    "\n",
    "rgb_c = convolutional_softmax_classifier(ksize, in_channels = 3)\n",
    "\n",
    "with tf.Session(graph = rgb_c.graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    fd        = {rgb_c.input_image : equalized_image[np.newaxis, :, :, :],\n",
    "                 rgb_c.input_mask  : raw_mask[np.newaxis, :, :] }\n",
    "    \n",
    "    for step in range(training_steps):\n",
    "        _, l = session.run([rgb_c.optimize, rgb_c.loss], feed_dict = fd)\n",
    "        if step % 500 == 0:\n",
    "            print('Loss at step {:^3d}: {:^5.2f}'.format(step, l))\n",
    "            \n",
    "    p_rgb = session.run(rgb_c.predictions, feed_dict = fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(p_rgb[0, :, :, 1], cmap = 'gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenfilters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective now is to compute the covariance matrix of this image patch. A problem is that we are working with a 20x20x3 patch, for which the corresponding covariance matrix contains 1.6 million elements. Our purpose in deriving the covariance matrix is to compute its eigenvectors (i.e. the principal components of the rust texel in intensity space).\n",
    "\n",
    "We could conceivably move to a different colour space to reduce the number of elements (e.g. grayscale or a channel of HSL or RGB), however in doing this we would lose information that may be valuable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can do with a 3x3 patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patch_size = 7\n",
    "patches    = [ rust_element[i:i+patch_size, j:j + patch_size, :].flatten()\n",
    "               for i in range(rust_element.shape[0] - patch_size)\n",
    "               for j in range(rust_element.shape[1] - patch_size) ]\n",
    "patches    = np.stack(patches, axis = 0)\n",
    "\n",
    "C = np.cov(patches, rowvar = False)\n",
    "eigenvalues, principal_components = np.linalg.eig(C)\n",
    "\n",
    "plt.figure(figsize = (15, 5))\n",
    "plt.hist(eigenvalues, bins = 21); plt.xlabel('Component standard deviation'); plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that one component points in a direction , with 18 components pointing in directions with little variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.signal import convolve, gaussian\n",
    "\n",
    "def apply_smoothing(image, ksize = 5):\n",
    "    kernel_size = ksize\n",
    "    n_std       = 3\n",
    "    K           = np.tensordot(gaussian(kernel_size, std = kernel_size/n_std)[:, np.newaxis],\n",
    "                               gaussian(kernel_size, std = kernel_size/n_std)[np.newaxis, :],\n",
    "                               axes = [1, 0])\n",
    "    return convolve(image, K, mode = 'same')\n",
    "\n",
    "def apply_filters(image, filters):\n",
    "    \"\"\"\n",
    "        Filters is a list of filter kernels to be convolved with an image.\n",
    "        Returns a stack of image channels, with each channel corresponding to its respective filter.\n",
    "    \"\"\"\n",
    "    return [convolve(image.astype(np.float32), f, mode = 'valid').squeeze() for f in filters]\n",
    "\n",
    "n_filters  = 10\n",
    "eigfilters = [principal_components[i].reshape([patch_size, patch_size, 3])\n",
    "              for i in np.flip(eigenvalues.argsort(), axis = 0)[:n_filters]]\n",
    "\n",
    "filtered_images = [np.pad(apply_smoothing(img)**2, [[patch_size//2, patch_size//2]]*2, mode = 'reflect')\n",
    "                   for img in apply_filters(raw_image, eigfilters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(n_filters, 2, figsize = (15, 25))\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    if i % 2 == 0:\n",
    "        ax.imshow(raw_image)\n",
    "        ax.axis('off');\n",
    "    else:\n",
    "        ax.imshow(filtered_images[i//2], cmap = 'gray')\n",
    "        ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eigfilter_input = np.stack(filtered_images, axis = -1)\n",
    "eig_c = convolutional_softmax_classifier(ksize, in_channels = eigfilter_input.shape[-1])\n",
    "\n",
    "with tf.Session(graph = eig_c.graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    fd        = {eig_c.input_image : eigfilter_input[np.newaxis, :, :, :],\n",
    "                 eig_c.input_mask  : raw_mask[np.newaxis, :, :] }\n",
    "    \n",
    "    for step in range(training_steps):\n",
    "        _, l = session.run([eig_c.optimize, eig_c.loss], feed_dict = fd)\n",
    "        if step % 500 == 0:\n",
    "            print('Loss at step {:^3d}: {:^5.2f}'.format(step, l))\n",
    "            \n",
    "    p_eig = session.run(eig_c.predictions, feed_dict = fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(p_eig[0, patch_size//2:-patch_size//2, patch_size//2:-patch_size//2, 0], cmap = 'gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laws' filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L5 = np.array([1, 4, 6, 4, 1])\n",
    "E5 = np.array([-1, 2, 0, 2, 1])\n",
    "S5 = np.array([1, 0, 2, 0, -1])\n",
    "R5 = np.array([1, -4, 6, -4, 1])\n",
    "W5 = np.array([-1, 2, 0, -2, 1])\n",
    "\n",
    "Laws_filters = [np.tensordot(f1[:, np.newaxis], f2[np.newaxis, :], axes = [1, 0])\n",
    "                for f1 in [L5, E5, S5, R5, W5]\n",
    "                for f2 in [L5, E5, S5, R5, W5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(5, 5, figsize = (15, 15))\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.imshow(Laws_filters[i], cmap = 'gray')\n",
    "    ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Laws_filters = [np.stack([f, f, f], axis = -1) for f in Laws_filters]\n",
    "\n",
    "filtered_images = [np.pad(apply_smoothing(img)**2, [[patch_size//2, patch_size//2]]*2, mode = 'reflect')\n",
    "                   for img in apply_filters(equalized_image, Laws_filters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(len(Laws_filters), 2, figsize = (15, 50))\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    if i % 2 == 0:\n",
    "        ax.imshow(equalized_image)\n",
    "        ax.axis('off');\n",
    "    else:\n",
    "        ax.imshow(filtered_images[i//2], cmap = 'gray')\n",
    "        ax.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(filtered_images[-2], cmap = 'gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(filtered_images[-2][1:-1, 1:-1]*-equalized_image[:,:,0], cmap = 'gray')\n",
    "plt.axis('off');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
