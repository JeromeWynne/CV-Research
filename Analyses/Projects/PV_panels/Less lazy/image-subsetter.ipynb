{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Subsetter\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import disk\n",
    "from skimage.filters import rank\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "import uuid\n",
    "\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_image_id(image_filepath):\n",
    "    image_name   = image_filepath.split('\\\\')[-1]\n",
    "    image_id_str = image_name[:-4]\n",
    "    return image_id_str\n",
    "\n",
    "\n",
    "def get_mask_from_id(image_id, masked_image_filepaths):\n",
    "    # Return annotation image associated with image_id\n",
    "    ann_ix = [get_image_id(fp) for fp in masked_image_filepaths].index(image_id)\n",
    "    return imread(masked_image_filepaths[ann_ix])\n",
    "\n",
    "\n",
    "def resize_image_and_mask(image, mask, dst_size):\n",
    "    image = resize(image, dst_size, order = 0)\n",
    "    mask  = resize(mask, dst_size, order  = 0)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def train_validation_test_split(X, y, frac_train=0.6, frac_validate=0.15, frac_test=0.25, rng = 1):\n",
    "    try:\n",
    "        assert sum([frac_train, frac_validate, frac_test]) == 1\n",
    "    except AssertionError:\n",
    "        raise ValueError('The training, test, and validation fractions do not sum to 1.')\n",
    "        \n",
    "    frac_train = frac_train/(frac_train + frac_validate)\n",
    "    X_train, X_test, y_train, y_test  = train_test_split(X, y, train_size = 1 - frac_test,\n",
    "                                                         test_size = frac_test, random_state = rng,\n",
    "                                                         stratify = y)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size = frac_train,\n",
    "                                                          test_size = 1 - frac_train, random_state = rng,\n",
    "                                                          stratify = y_train)\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test\n",
    "\n",
    "\n",
    "def pickle_data(dataset, labels, name):\n",
    "    pickle.dump(dataset, open('.\\\\data\\\\' + name + '_dataset.p', 'wb'))\n",
    "    pickle.dump(labels, open('.\\\\data\\\\' + name + '_labels.p', 'wb'))\n",
    "    print('Data pickled successully.')\n",
    "    \n",
    "    \n",
    "def augment_image(img):\n",
    "    flip_v     = np.random.randint(0, 2)\n",
    "    flip_h     = np.random.randint(0, 2)\n",
    "    scale_f    = np.random.uniform(1/1.02, 1) # Some of the cracks are at the extremes of the frame\n",
    "    tform      = AffineTransform(scale = (scale_f, scale_f))\n",
    "    aug_img    = warp(img, tform)\n",
    "    if flip_v: aug_img = np.flip(aug_img, axis = 0)\n",
    "    if flip_h: aug_img = np.flip(aug_img, axis = 1)\n",
    "    aug_img    = rank.equalize(aug_img, selem=disk(7)).astype(float)\n",
    "    aug_img    = (aug_img - 128)/128 # skimage bumps the images from [0, 1] to [0, 255]\n",
    "    return aug_img\n",
    "\n",
    "\n",
    "def balance_dataset(dataset, labels):\n",
    "    n_labels    = len(np.unique(labels))\n",
    "    min_count   = min([sum(labels == class_label) for class_label in np.unique(labels)])\n",
    "    max_count   = min_count*4\n",
    "    bal_dataset = np.zeros([max_count*n_labels, dataset.shape[1]])\n",
    "    bal_labels  = np.zeros([max_count*n_labels])\n",
    "    ix = 0\n",
    "    for class_label in np.unique(labels):\n",
    "        for _ in range(max_count):\n",
    "            class_ix = np.nonzero(labels == class_label)[0]\n",
    "            rnd_ix   = np.random.choice(class_ix)\n",
    "            bal_dataset[ix, :] = augment_image(dataset[rnd_ix, :].reshape(kernel_size)).reshape(kernel_size[0]*kernel_size[1])\n",
    "            bal_labels[ix]     = labels[rnd_ix]\n",
    "            ix += 1\n",
    "    print('Dataset successfully balanced.')\n",
    "    return bal_dataset, bal_labels\n",
    "\n",
    "def shuffle_dataset(dataset, labels):\n",
    "    shuffle_ix = np.random.choice(range(labels.shape[0]), labels.shape[0], replace = False)\n",
    "    dataset    = np.array([dataset[ix, :] for ix in shuffle_ix])\n",
    "    labels     = np.array([labels[ix] for ix in shuffle_ix])\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 52/52 [00:03<00:00, 13.30it/s]\n"
     ]
    }
   ],
   "source": [
    "image_filepaths = glob.glob('.\\\\data\\\\resized-images\\\\*.png')\n",
    "mask_filepaths  = glob.glob('.\\\\data\\\\annotated-images-masks\\\\*.png')\n",
    "\n",
    "image_size    = [608, 608]\n",
    "kernel_size = [128, 128]\n",
    "step_size   = [40, 40]\n",
    "\n",
    "n_cols    = np.floor((image_size[1] - kernel_size[1])/step_size[1])\n",
    "n_rows    = np.floor((image_size[0] - kernel_size[0])/step_size[0])\n",
    "n_patches = len(image_filepaths)*(n_rows*n_cols).astype(int)\n",
    "\n",
    "dataset   = np.zeros([n_patches, kernel_size[0], kernel_size[1]])\n",
    "masks     = np.zeros([n_patches, kernel_size[0], kernel_size[1]])\n",
    "labels    = np.zeros(n_patches)\n",
    "ids       = np.array([None]*n_patches)\n",
    "ix        = 0\n",
    "\n",
    "for fp in tqdm(image_filepaths):\n",
    "    image       = imread(fp)\n",
    "    image_id    = get_image_id(fp)\n",
    "    mask        = get_mask_from_id(image_id, mask_filepaths)\n",
    "    image, mask = resize_image_and_mask(image, mask, image_size)\n",
    "    for r in range(0, image_size[0] - kernel_size[0], step_size[0]):\n",
    "        for c in range(0, image_size[1] - kernel_size[1], step_size[1]):\n",
    "            image_patch       = image[r:r + kernel_size[0], c:c + kernel_size[1]]\n",
    "            mask_patch        = mask[r:r + kernel_size[0], c:c + kernel_size[1]]\n",
    "            dataset[ix, :, :] = image_patch\n",
    "            masks[ix, :, :]   = mask_patch\n",
    "            labels[ix]        = np.heaviside(np.sum(mask_patch) - 40, 0)\n",
    "            ids[ix]           = uuid.uuid4().hex\n",
    "            ix += 1\n",
    "\n",
    "dataset = dataset.reshape([dataset.shape[0], kernel_size[0]*kernel_size[0]])\n",
    "masks = masks.reshape([masks.shape[0], kernel_size[0]*kernel_size[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images subset and split successfully.\n"
     ]
    }
   ],
   "source": [
    "(train_dataset, valid_dataset,\n",
    " test_dataset, train_labels, \n",
    " valid_labels, test_labels) = train_validation_test_split(dataset, labels)\n",
    "print('Images subset and split successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pickled successully.\n",
      "Data pickled successully.\n",
      "Testing and validation sets successfully pickled.\n"
     ]
    }
   ],
   "source": [
    "pickle_data(2*(valid_dataset - 1), valid_labels, 'valid')\n",
    "pickle_data(2*(test_dataset  - 1), test_labels, 'test')\n",
    "print('Testing and validation sets successfully pickled.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py:110: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully balanced.\n"
     ]
    }
   ],
   "source": [
    "btrain_dataset, btrain_labels = balance_dataset(train_dataset, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pickled successully.\n",
      "Training set successfully pickled.\n"
     ]
    }
   ],
   "source": [
    "btrain_dataset, btrain_labels = shuffle_dataset(btrain_dataset, btrain_labels)\n",
    "pickle_data(btrain_dataset, btrain_labels, 'train')\n",
    "print('Training set successfully pickled.')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
