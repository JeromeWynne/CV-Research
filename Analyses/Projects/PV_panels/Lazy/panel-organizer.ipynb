{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Author: Jerome Wynne (Jerome.Wynne.2014@bristol.ac.uk)*\n",
    "\n",
    "*Last updated: 22/06/2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel Resizer\n",
    "---\n",
    "This script resizes the PV panel images to a fixed size. It drops three images in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# Python 3.5 should be used\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images resized sucessfully.\n"
     ]
    }
   ],
   "source": [
    "# Read in images\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from scipy.misc import imread, imresize, imsave\n",
    "from skimage.util import random_noise\n",
    "from scipy.misc import imread, imsave\n",
    "from skimage.morphology import disk\n",
    "from skimage.filters import rank\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "force = True # Controls whether to overwrite the output of previous runs\n",
    "\n",
    "raw_image_fps    = glob.glob('.\\\\data\\\\raw-images\\*.png')\n",
    "raw_images       = [imread(fp) for fp in raw_image_fps]\n",
    "image_ids        = range(len(raw_images))\n",
    "\n",
    "# Sort images by size then drop the first three images, which are much smaller\n",
    "image_ids  = sorted(image_ids,  key = lambda j: raw_images[j].shape)[3:] # Sort the ids in the same way as the images\n",
    "raw_images = sorted(raw_images, key = np.shape)[3:]\n",
    "\n",
    "# Resize images and save them to a new folder\n",
    "if not os.path.exists('.\\\\data\\\\resized-images'):\n",
    "    os.makedirs('.\\\\data\\\\resized-images')\n",
    "elif force:\n",
    "    shutil.rmtree('.\\\\data\\\\resized-images')\n",
    "    os.makedirs('.\\\\data\\\\resized-images')\n",
    "\n",
    "# Resize images to median image size\n",
    "dst_dim        = np.median([img.shape[0] for img in raw_images]).astype(int)\n",
    "resized_images = [imresize(img, [dst_dim, dst_dim]) for img in raw_images]\n",
    "\n",
    "# Write images to local directory\n",
    "for j in range(len(resized_images)):\n",
    "    imsave('.\\\\data\\\\resized-images\\\\resized_' + str(image_ids[j]) + '.png', resized_images[j])\n",
    "    \n",
    "print('Images resized sucessfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel Subsetter\n",
    "---\n",
    "The script below writes out 6x6 grids of subimages for each resized image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images subset successfully.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('.\\\\data\\\\subset-images'):\n",
    "    os.makedirs('.\\\\data\\\\subset-images')\n",
    "elif force:\n",
    "    shutil.rmtree('.\\\\data\\\\subset-images')\n",
    "    os.makedirs('.\\\\data\\\\subset-images')\n",
    "    \n",
    "patch_dim = np.floor(dst_dim/6).astype(int) # Divide the images into 6x6 patches\n",
    "\n",
    "for k in range(0, len(resized_images)):\n",
    "    for h in range(0, 6): # Horizontal grid element variable\n",
    "        for v in range(0, 6): # Vertical grid element variable\n",
    "            sub_image = resized_images[k][v*patch_dim:(v + 1)*patch_dim,\n",
    "                                          h*patch_dim:(h + 1)*patch_dim]\n",
    "            imsave('.\\\\data\\\\subset-images\\\\subset_' + str(image_ids[k]) + '_' + str(v) + str(h) + '.png', sub_image)\n",
    "            \n",
    "print('Images subset successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel Train-Test-Splitter\n",
    "---\n",
    "This script performs a 50:50 stratified split of the subset data into training and testing images. 105 crack images, 1695 no-crack images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images split into training, testing, and validation sets successfully.\n"
     ]
    }
   ],
   "source": [
    "# Read in image labels\n",
    "image_info = pd.read_csv('.\\\\data\\\\labels\\\\subset-cracked.csv')\n",
    "train_img_fp, test_img_fp, train_labels, test_labels = train_test_split(image_info['filename'], image_info['cracked'],\n",
    "                                                                         train_size = 0.5, stratify = image_info['cracked'],\n",
    "                                                                          random_state = 1)\n",
    "train_img_fp, valid_img_fp, train_labels, valid_labels = train_test_split(train_img_fp, train_labels,\n",
    "                                                                          random_state = 1, stratify = train_labels)\n",
    "\n",
    "# Make directories for training and testing image sets\n",
    "if not os.path.exists('.\\\\data\\\\subset-images\\\\training'):\n",
    "    os.makedirs('.\\\\data\\\\subset-images\\\\training')\n",
    "if not os.path.exists('.\\\\data\\\\subset-images\\\\testing'):\n",
    "    os.makedirs('.\\\\data\\\\subset-images\\\\testing')\n",
    "if not os.path.exists('.\\\\data\\\\subset-images\\\\testing'):\n",
    "    os.makedirs('.\\\\data\\\\subset-images\\\\validation')\n",
    "elif force:\n",
    "    shutil.rmtree('.\\\\data\\\\subset-images\\\\training')\n",
    "    shutil.rmtree('.\\\\data\\\\subset-images\\\\testing')\n",
    "    os.makedirs('.\\\\data\\\\subset-images\\\\testing')\n",
    "    os.makedirs('.\\\\data\\\\subset-images\\\\training')\n",
    "    os.makedirs('.\\\\data\\\\subset-images\\\\validation')\n",
    "    \n",
    "# Move the training and testing images to their respective folders\n",
    "for fp in train_img_fp:  os.rename(\".\\\\data\\\\subset-images\\\\\" + fp, '.\\\\data\\\\subset-images\\\\training\\\\' + fp)\n",
    "for fp in test_img_fp:   os.rename(\".\\\\data\\\\subset-images\\\\\" + fp, '.\\\\data\\\\subset-images\\\\testing\\\\' + fp)\n",
    "for fp in valid_img_fp:  os.rename(\".\\\\data\\\\subset-images\\\\\" + fp, '.\\\\data\\\\subset-images\\\\validation\\\\' + fp)\n",
    "\n",
    "print('Images split into training, testing, and validation sets successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set Augmenter\n",
    "\n",
    "The following script augments the training set to create a balanced set of responses. It writes the generated images and their associated labels to the hard disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation and training images pickled successfully.\n"
     ]
    }
   ],
   "source": [
    "def augment_image(img):\n",
    "    flip_v     = np.random.randint(0, 2)\n",
    "    flip_h     = np.random.randint(0, 2)\n",
    "    noise      = np.random.randint(0, 2)\n",
    "    scale_f    = np.random.uniform(0.98, 1.02) # Some of the cracks are at the extremes of the frame\n",
    "    tform      = AffineTransform(scale = (scale_f, scale_f))\n",
    "    aug_img    = warp(img, tform)\n",
    "    if flip_v: aug_img = np.flip(aug_img, axis = 0)\n",
    "    if flip_h: aug_img = np.flip(aug_img, axis = 1)\n",
    "    if noise:  aug_img = random_noise(aug_img, 'gaussian', var = 0.001)\n",
    "    aug_img    = rank.equalize(img, selem=disk(100))\n",
    "    aug_img    = aug_img - 0.5 # skimage bumps the images from [0, 255] to [0, 1]\n",
    "    return aug_img\n",
    "\n",
    "def pickle_dataset(list_of_img_fps, list_of_labels, name, augment = False):\n",
    "    if augment:\n",
    "        img_array = np.array([augment_image(imread(fp)) for fp in list_of_img_fps])\n",
    "    else:\n",
    "        img_array = np.array([imread(fp) for fp in list_of_img_fps])\n",
    "    pickle.dump(img_array, open('.\\\\data\\\\' + name + '_dataset.p', 'wb'))\n",
    "    pickle.dump(list_of_labels, open('.\\\\data\\\\' + name + '_labels.p', 'wb'))\n",
    "\n",
    "def shuffle_data(dataset, labels):\n",
    "    if type(dataset) != list: dataset = list(dataset)\n",
    "    if type(labels)  != list: labels  = list(labels)\n",
    "    shuffle_ix = np.random.choice(range(len(labels)), len(labels), replace = False)\n",
    "    dataset    = [dataset[ix] for ix in shuffle_ix]\n",
    "    labels     = [labels[ix] for ix in shuffle_ix]\n",
    "    return dataset, labels\n",
    "    \n",
    "# Balance training data\n",
    "n_images_per_class = sum(train_labels == 0)\n",
    "uncracked_img_fp   = train_img_fp[train_labels == 0].sample(n_images_per_class, replace = False)\n",
    "cracked_img_fp     = train_img_fp[train_labels == 1].sample(n_images_per_class, replace = True)\n",
    "aug_train_fp       = list(pd.concat([uncracked_img_fp, cracked_img_fp]))\n",
    "aug_train_labels   = [0]*n_images_per_class + [1]*n_images_per_class\n",
    "\n",
    "# Shuffle the training data\n",
    "aug_train_fp       = ['.\\\\data\\\\subset-images\\\\training\\\\' + fp for fp in aug_train_fp]\n",
    "valid_img_fp       = ['.\\\\data\\\\subset-images\\\\validation\\\\' + fp for fp in valid_img_fp]\n",
    "aug_train_fp, aug_train_labels = shuffle_data(aug_train_fp, aug_train_labels)\n",
    "valid_img_fp, valid_labels     = shuffle_data(valid_img_fp, valid_labels)\n",
    "\n",
    "# Pickle the dataset and labels to the disk as an array and list respectively\n",
    "pickle_dataset(aug_train_fp, aug_train_labels, 'training', augment = True)\n",
    "pickle_dataset(valid_img_fp, valid_labels, 'validation', augment = False)\n",
    "\n",
    "print('Validation and training images pickled successfully.')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
