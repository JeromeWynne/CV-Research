{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Import data.\n",
    "# 2. Set up a small graph.\n",
    "# 3. Train the model against it.\n",
    "# 4. Save the model.\n",
    "# 5. Restore and continue to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import data (it has already been scaled and centered).\n",
    "image_size  = data['train'].shape[1] # 20\n",
    "n_channels  = 1\n",
    "\n",
    "def reformat(data): # Adds the channel dimension to the numpy arrays\n",
    "    data = data.reshape( (-1, image_size, image_size, n_channels))  # -1 implies the dimension is inferred\n",
    "    return data\n",
    "\n",
    "def load_pickled_data(name):\n",
    "    data   = pickle.load(open(name+'_dataset.p', 'rb'))\n",
    "    labels = pickle.load(open(name+'_labels.p', 'rb'))\n",
    "    return reformat(data), labels\n",
    "\n",
    "data = {}; labels = {};\n",
    "\n",
    "data['train'], labels['train'] = load_pickled_data('../data/train')\n",
    "data['valid'], labels['valid'] = load_pickled_data('../data/valid')\n",
    "data['test'], labels['test']   = load_pickled_data('../data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configure a simple 2-layer graph.\n",
    "\n",
    "batch_size  = 32\n",
    "n_classes   = 2\n",
    "output_channels = [64, 128, 1024]\n",
    "filter_size     = [3, 3]\n",
    "\n",
    "# 20 x 20 input\n",
    "# Convolve with 64 3 x 3 filters, stride 2 (out. image size of 10 x 10)\n",
    "# Convolve with 128 3 x 3 filters, stride 2 (out. image size of 5 x 5)\n",
    "# Fully connected layer (1024 nodes)\n",
    "# Softmax output\n",
    "\n",
    "simple_graph = tf.Graph()\n",
    "\n",
    "with simple_graph.as_default():\n",
    "    # Placeholders and constants\n",
    "    tf_train_data   = tf.placeholder(tf.float32, [batch_size, image_size, image_size, n_channels]) # BWHN\n",
    "    tf_train_labels = tf.placeholder(tf.float32, [batch_size, n_classes])\n",
    "    tf_valid_data   = tf.constant(data['valid'])\n",
    "    tf_test_data    = tf.constant(data['test'])\n",
    "    \n",
    "    # Variables\n",
    "    filters1 = tf.Variable(tf.truncated_normal(\n",
    "                    [filter_size[0], filter_size[0], n_channels, output_channels[0]], stddev = 0.01))\n",
    "    biases1  = tf.Variable(tf.zeros([output_channels[0]]))\n",
    "    \n",
    "    filters2 = tf.Variable(tf.truncated_normal(\n",
    "                    [filter_size[1], filter_size[1], output_channels[0], output_channels[1]], stddev = 0.01))\n",
    "    biases2  = tf.Variable(tf.zeros([output_channels[1]]))\n",
    "    \n",
    "    weights3 = tf.Variable(tf.truncated_normal(\n",
    "                    [output_channels[1] * image_size // 4 * image_size // 4, n_classes], stddev = 0.01))\n",
    "    biases3  = tf.Variable(tf.zeros(n_classes))\n",
    "    \n",
    "    def model(data):\n",
    "        # Layer 1 : 20 x 20 x 1 input ; 10 x 10 x 64 output ; (3 x 3 x 1) x 64 filters ; stride of 2 ; same padding\n",
    "        conv = tf.nn.conv2d(data, filters1, strides = [1, 2, 2, 1], padding = 'SAME', use_cudnn_on_gpu = True)\n",
    "        act  = tf.nn.relu(conv + biases1)\n",
    "        \n",
    "        # Layer 2 : 10 x 10 x 64 input ; 5 x 5 x 128 output ; (3 x 3 x 64) x 128 filters ; stride of 2 ; same padding\n",
    "        conv = tf.nn.conv2d(act, filters2, strides = [1, 2, 2, 1], padding = 'SAME', use_cudnn_on_gpu = True)\n",
    "        act  = tf.nn.relu(conv + biases2)\n",
    "        \n",
    "        # layer 3 : fully connected ; 5*5*128 input ; (5*5*128 x 2) filters\n",
    "        shape  = act.get_shape().as_list()\n",
    "        act    = tf.reshape(act, [shape[0], shape[1]*shape[2]*shape[3]])\n",
    "        logits = tf.nn.relu(tf.matmul(act, weights3) + biases3)\n",
    "        return logits\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    logits = model(tf_train_data)\n",
    "    loss   = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tf_train_labels))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "    \n",
    "    # Feedback information\n",
    "    tf_train_predictions = tf.nn.softmax(logits)\n",
    "    tf_valid_predictions = tf.nn.softmax(model(tf_valid_data))\n",
    "    tf_test_predictions  = tf.nn.softmax(model(tf_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "------------\n",
      "Minibatch loss: 000.69\n",
      "\n",
      "Minibatch accuracy: 100.00%\n",
      "\n",
      "Validation accuracy: 48.88%\n",
      "\n",
      "\n",
      "Step 2000\n",
      "------------\n",
      "Minibatch loss: 000.48\n",
      "\n",
      "Minibatch accuracy: 100.00%\n",
      "\n",
      "Validation accuracy: 76.00%\n",
      "\n",
      "\n",
      "Step 4000\n",
      "------------\n",
      "Minibatch loss: 000.50\n",
      "\n",
      "Minibatch accuracy: 75.00%\n",
      "\n",
      "Validation accuracy: 81.29%\n",
      "\n",
      "\n",
      "Step 6000\n",
      "------------\n",
      "Minibatch loss: 000.71\n",
      "\n",
      "Minibatch accuracy: 75.00%\n",
      "\n",
      "Validation accuracy: 79.25%\n",
      "\n",
      "\n",
      "Step 8000\n",
      "------------\n",
      "Minibatch loss: 000.14\n",
      "\n",
      "Minibatch accuracy: 96.88%\n",
      "\n",
      "Validation accuracy: 83.33%\n",
      "\n",
      "\n",
      "Step 10000\n",
      "------------\n",
      "Minibatch loss: 000.25\n",
      "\n",
      "Minibatch accuracy: 84.38%\n",
      "\n",
      "Validation accuracy: 84.42%\n",
      "\n",
      "\n",
      "Testing accuracy @ 10001 steps: 82.33%\n"
     ]
    }
   ],
   "source": [
    "# 3. Create and run a session.\n",
    "\n",
    "def accuracy_score(pred, true):\n",
    "    return 100*np.sum(np.equal(np.argmax(pred, axis = 1), np.argmax(true, axis = 1)))/pred.shape[0]\n",
    "\n",
    "def minibatch(data, labels, batch_size, step):\n",
    "    posn         = (step * batch_size) % (data.shape[0] - batch_size)\n",
    "    batch_data   = data[posn:(posn + batch_size), :, :, :]\n",
    "    batch_labels = labels[posn:(posn + batch_size), :]\n",
    "    return batch_data, batch_labels\n",
    "\n",
    "def performance_report(step, loss, train_predictions, train_labels):\n",
    "    print('Step {}\\n------------'.format(step))\n",
    "    print('Minibatch loss: {:06.2f}\\n'.format(loss))\n",
    "    print('Minibatch accuracy: {:04.2f}%\\n'.format(accuracy_score(train_predictions, train_labels)))\n",
    "    print('Validation accuracy: {:04.2f}%\\n\\n'.format(accuracy_score(tf_valid_predictions.eval(), labels['valid'])))\n",
    "\n",
    "\n",
    "n_steps = 10001\n",
    "\n",
    "with tf.Session(graph = simple_graph) as session:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        batch_data, batch_labels = minibatch(data['train'], labels['train'], batch_size, step)\n",
    "        fd = {tf_train_data:batch_data, tf_train_labels:batch_labels}\n",
    "        \n",
    "        _, l, batch_predictions  = session.run([optimizer, loss, tf_train_predictions], feed_dict = fd)\n",
    "        \n",
    "        if step % 2000 == 0:\n",
    "            performance_report(step, l, batch_predictions, batch_labels)\n",
    "            \n",
    "    print('Testing accuracy @ {} steps: {:04.2f}%'.format(n_steps,\n",
    "                                                          accuracy_score(tf_test_predictions.eval(), labels['test'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
