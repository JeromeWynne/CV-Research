{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Author: Jerome Wynne (Jerome.Wynne.2014@bristol.ac.uk)*\n",
    "\n",
    "*Last updated: 22/06/2017*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel Resizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script resizes the PV panel images to a fixed size. It drops three images in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# Python 3.5 should be used\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(489, 479),\n",
       " (775, 772),\n",
       " (776, 773),\n",
       " (959, 967),\n",
       " (963, 954),\n",
       " (963, 956),\n",
       " (963, 966),\n",
       " (965, 962),\n",
       " (966, 962),\n",
       " (966, 962),\n",
       " (966, 963),\n",
       " (966, 964),\n",
       " (966, 964),\n",
       " (966, 965),\n",
       " (966, 965),\n",
       " (967, 956),\n",
       " (967, 958),\n",
       " (967, 963),\n",
       " (967, 964),\n",
       " (967, 964),\n",
       " (968, 955),\n",
       " (968, 956),\n",
       " (968, 963),\n",
       " (968, 964),\n",
       " (968, 965),\n",
       " (968, 965),\n",
       " (969, 955),\n",
       " (969, 957),\n",
       " (970, 965),\n",
       " (970, 973),\n",
       " (972, 963),\n",
       " (972, 964),\n",
       " (973, 963),\n",
       " (973, 963),\n",
       " (973, 963),\n",
       " (973, 963),\n",
       " (973, 963),\n",
       " (973, 963),\n",
       " (973, 964),\n",
       " (973, 964),\n",
       " (973, 964),\n",
       " (973, 964),\n",
       " (973, 964),\n",
       " (974, 962),\n",
       " (974, 962),\n",
       " (974, 963),\n",
       " (974, 963),\n",
       " (974, 963),\n",
       " (974, 963),\n",
       " (976, 962),\n",
       " (978, 963),\n",
       " (981, 963)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get image filepaths\n",
    "import glob\n",
    "raw_image_fps = glob.glob('.\\\\data\\\\raw-images\\*.png')\n",
    "\n",
    "# Read in images\n",
    "from scipy.misc import imread, imresize, imsave\n",
    "import numpy as np\n",
    "raw_images = [imread(fp) for fp in raw_image_fps]\n",
    "image_ids  = np.arange(0, len(raw_images)) # These will come in handy later\n",
    "\n",
    "# Get image dimensions\n",
    "image_dimensions = [np.shape(img) for img in raw_images]\n",
    "\n",
    "# Print image dimensions sorted by height\n",
    "sorted(image_dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is apparent from the above, there are three images in the dataset that are smaller than the others. I'm concerned that rescaling these images might impede the ability of a classifier to identify damage and will drop them (for the time being, at least)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort images by size then drop the first three images\n",
    "image_ids  = sorted(image_ids,  key = lambda j: np.shape(raw_images[j]))[3:] # Sort the ids in the same way as the images\n",
    "raw_images = sorted(raw_images, key = np.shape)[3:]\n",
    "\n",
    "# Resize images and save them to a new folder\n",
    "import os\n",
    "if not os.path.exists('.\\\\data\\\\resized-images'):\n",
    "    os.makedirs('.\\\\data\\\\resized-images')\n",
    "\n",
    "# Get destination image size (we'll take the mean of the image dimensions)\n",
    "dst_dim  = np.ceil(np.mean(np.asarray(image_dimensions), axis = 0)).astype(int)\n",
    "dst_dim[1] = dst_dim[0] # To simplify matters, we make the images square\n",
    "\n",
    "# Resize images\n",
    "resized_images = [imresize(img, dst_dim) for img in raw_images]\n",
    "\n",
    "# Write images to local directory\n",
    "for j in range(len(resized_images)):\n",
    "    imsave('.\\\\data\\\\resized-images\\\\resized_' + str(image_ids[j]) + '.png', resized_images[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resized images should now be available locally. \n",
    "\n",
    "# Panel Subsetter\n",
    "\n",
    "The script below writes out 6x6 grids of subimages for each resized image. If this doesn't work, I'll investigate using sliding window to subset the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.misc import imsave\n",
    "\n",
    "if not os.path.exists('.\\\\data\\\\subset-images'):\n",
    "    os.makedirs('.\\\\data\\\\subset-images')\n",
    "    \n",
    "sub_dim = np.floor(dst_dim[1]/6).astype(int)\n",
    "for k in range(len(resized_images)):\n",
    "    for i in range(0, 6): # Horizontal grid element variable\n",
    "        for j in range(0, 6): # Vertical grid element variable\n",
    "            sub_image = resized_images[k][j*sub_dim : (j+1)*sub_dim, i*sub_dim:(i+1)*sub_dim]\n",
    "            imsave('.\\\\data\\\\subset-images\\\\subset_' + str(image_ids[k]) + '_' + str(j) + str(i) + '.png', sub_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panel Train-Test-Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script performs a 50:50 stratified split of the subset data into training and testing images. 105 crack images, 1695 no-crack images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in image labels\n",
    "import pandas as pd\n",
    "image_labels = pd.read_csv('.\\\\data\\\\labels\\\\subset-cracked.csv')\n",
    "image_labels.head()\n",
    "\n",
    "# Split them\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "from sklearn.model_selection import train_test_split\n",
    "fp_train, fp_test, labels_train, labels_test = train_test_split(image_labels['filename'], image_labels['cracked'],\n",
    "                                                                         train_size = 0.5, stratify = image_labels['cracked'],\n",
    "                                                                               random_state = 1)\n",
    "\n",
    "# Make directories for training and testing image sets\n",
    "import os\n",
    "if not os.path.exists('.\\\\data\\\\subset-images\\\\training'):\n",
    "    os.makedirs('.\\\\data\\\\subset-images\\\\training')\n",
    "if not os.path.exists('.\\\\data\\\\subset-images\\\\testing'):\n",
    "    os.makedirs('.\\\\data\\\\subset-images\\\\testing')\n",
    "\n",
    "# Move the images to their respective folders\n",
    "for fp in fp_train:\n",
    "    os.rename(\".\\\\data\\\\subset-images\\\\\" + fp, '.\\\\data\\\\subset-images\\\\training\\\\' + fp)\n",
    "for fp in fp_test:\n",
    "    os.rename(\".\\\\data\\\\subset-images\\\\\" + fp, '.\\\\data\\\\subset-images\\\\testing\\\\' + fp)\n",
    "    \n",
    "# Save the subset labels\n",
    "pd.concat([fp_train, labels_train], axis = 1).to_csv('.\\\\data\\\\labels\\\\subset-training-labels-cracked.csv', index = False)\n",
    "pd.concat([fp_test, labels_test], axis = 1).to_csv('.\\\\data\\\\labels\\\\subset-testing-labels-cracked.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set Augmenter\n",
    "\n",
    "The following script augments the training set to create a balanced set of responses. It writes the generated images and their associated labels to the hard disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imread, imsave\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from skimage.util import random_noise\n",
    "\n",
    "def augment_image(img_fp, dst_fp):\n",
    "    img        = imread(img_fp)\n",
    "    flip_v     = np.random.randint(0, 2)\n",
    "    flip_h     = np.random.randint(0, 2)\n",
    "    scale_f    = np.random.uniform(0.98, 1.02) # Some of the cracks are the extremes of the frame\n",
    "    rotate_f   = np.random.uniform(-np.pi/36, np.pi/36)\n",
    "    tform      = AffineTransform(scale = (scale_f, scale_f), rotation = rotate_f)\n",
    "    aug_img    = warp(img, tform)\n",
    "    if flip_v: aug_img = np.flip(aug_img, axis = 0)\n",
    "    if flip_h: aug_img = np.flip(aug_img, axis = 1)\n",
    "    aug_img    = random_noise(aug_img, 'speckle', var = 0.01)\n",
    "    aug_img    = (aug_img*255/aug_img.max()).astype(int) # skimage bumps the images from [0, 255] to [0, 1]\n",
    "    imsave(dst_fp, aug_img)\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "# Read in training dataset [filename|cracked?]\n",
    "training_labels = pd.read_csv('.\\\\data\\\\labels\\\\subset-training-labels-cracked.csv')\n",
    "\n",
    "# Get the names of the images containing cracks only\n",
    "cracked_filenames = (training_labels.filename[training_labels['cracked'] == 1]).reset_index(drop = True)\n",
    "\n",
    "# Create a separate dataframe for the augmented images\n",
    "n_aug_images = len(training_labels) - sum(training_labels['cracked'] == 1)\n",
    "augmented_labels = pd.DataFrame(np.zeros([n_aug_images, 2]), columns = {'filename', 'cracked'})\n",
    "\n",
    "for j in range(n_aug_images):\n",
    "        ix = np.random.randint(0, len(cracked_filenames))\n",
    "        img_fp = '.\\\\data\\\\subset-images\\\\training\\\\' + cracked_filenames[ix]\n",
    "        aug_img_fp = img_fp[:-4] + '_aug_' + str(j) + '.png'\n",
    "        #augment_image(img_fp, aug_img_fp) # Augments and saves image, returns image filename\n",
    "        augmented_labels.loc[j] = [int(1), aug_img_fp[30:]]\n",
    "        \n",
    "aug_training_labels = pd.concat([training_labels, augmented_labels], axis = 0)\n",
    "aug_training_labels.to_csv('.\\\\data\\\\labels\\\\augmented-subset-training-cracked.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
