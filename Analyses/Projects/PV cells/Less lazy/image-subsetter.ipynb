{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Subsetter\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imread\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import glob\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 52/52 [00:16<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images subset successfully.\n"
     ]
    }
   ],
   "source": [
    "def get_image_id(image_filepath):\n",
    "    image_name   = image_filepath.split('\\\\')[-1]\n",
    "    image_id_str = image_name[:-4]\n",
    "    return image_id_str\n",
    "\n",
    "def get_mask_from_id(image_id, masked_image_filepaths):\n",
    "    # Return annotation image associated with image_id\n",
    "    ann_ix = [get_image_id(fp) for fp in masked_image_filepaths].index(image_id)\n",
    "    return imread(masked_image_filepaths[ann_ix])\n",
    "\n",
    "def resize_image_and_mask(image, mask, dst_size):\n",
    "    image = resize(image, dst_size, order = 0)\n",
    "    mask  = resize(mask, dst_size, order  = 0)\n",
    "    return image, mask\n",
    "\n",
    "# Write sliding-window subsets of images to training, test, and validation datasets (with labels)\n",
    "# Balance number of cracked images in training set by augmentation\n",
    "# 1. Read in image and mask.\n",
    "# 2. Slide window over mask.\n",
    "# 3. For each window frame, check the number of crack (i.e. white) pixels.\n",
    "# 4. If it is greater than 40px, label the image as being cracked.\n",
    "# 5. Split off a window from the image and append it to the big dataset.\n",
    "\n",
    "image_filepaths = glob.glob('.\\\\data\\\\resized-images\\\\*.png')\n",
    "mask_filepaths  = glob.glob('.\\\\data\\\\annotated-images-masks\\\\*.png')\n",
    "\n",
    "image_size    = [608, 608]\n",
    "kernel_size = [128, 128]\n",
    "step_size   = [30, 30]\n",
    "\n",
    "n_cols    = np.floor((image_size[1] - kernel_size[1])/step_size[1])\n",
    "n_rows    = np.floor((image_size[0] - kernel_size[0])/step_size[0])\n",
    "n_patches = len(image_filepaths)*(n_rows*n_cols).astype(int)\n",
    "\n",
    "dataset   = np.zeros([n_patches, kernel_size[0], kernel_size[1]])\n",
    "masks     = np.zeros([n_patches, kernel_size[0], kernel_size[1]])\n",
    "labels    = np.zeros(n_patches)\n",
    "ids       = np.array([None]*n_patches)\n",
    "ix        = 0\n",
    "\n",
    "for fp in tqdm(image_filepaths):\n",
    "    image       = imread(fp)\n",
    "    image_id    = get_image_id(fp)\n",
    "    mask        = get_mask_from_id(image_id, mask_filepaths)\n",
    "    image, mask = resize_image_and_mask(image, mask, image_size)\n",
    "    for r in range(0, image_size[0] - kernel_size[0], step_size[0]):\n",
    "        for c in range(0, image_size[1] - kernel_size[1], step_size[1]):\n",
    "            image_patch       = image[r:r + kernel_size[0], c:c + kernel_size[1]]\n",
    "            mask_patch        = mask[r:r + kernel_size[0], c:c + kernel_size[1]]\n",
    "            dataset[ix, :, :] = image_patch\n",
    "            masks[ix, :, :]   = mask_patch\n",
    "            labels[ix]        = np.heaviside(np.sum(mask_patch) - 40, 0)\n",
    "            ids[ix]           = uuid.uuid4().hex\n",
    "            ix += 1\n",
    "\n",
    "print('Images subset successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480.0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-146-bbebc67e583c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert 1 + 1 == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You now have the datasets.\n",
    "# Split them into train, test, cross_validation\n",
    "\n",
    "def train_validate_test_split(X, y, frac_train=0.6, frac_validate=0.15, frac_test=0.25, stratify=True):\n",
    "    try:\n",
    "        assert sum([frac_train, frac_validate, frac_test]) == 1\n",
    "    except AssertionError:\n",
    "        raise ValueError('The training, test, and validation fractions do not sum to 1.')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
