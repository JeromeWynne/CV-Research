{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factor Analysis for Face Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign a label indicating which of $M$ possible identities a face belongs to, based on a vector of grayscale pixel intensities $x$. Model the likelihood for each class using a factor analyzer:\n",
    "\n",
    "$$\n",
    "    \\text{Pr}(x \\ | \\ w_i = k) = \\text{Norm}_{x}(\\mu_k, \\phi_k\\phi_k^T + \\Sigma_k)\\\\[0.7em]\n",
    "$$\n",
    "Learn the parameters for the $k$th identity using the images of faces corresponding to that identity. Use expecation maximization to learn these parameters. Assign priors $\\text{Pr}(w = k)$ according to each face's prevalence in the database. To evaluate a new face image $x_i$, compute the posterior $\\text{Pr}(w_i|x_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data:            Continuous - vectors of grayscale pixel intensities.\n",
    "# World states:    Discrete - categories corresponding to face identities.\n",
    "# Model:           Generative - factor analyzers each parameterized by unique means, factors, and diagonal covariance matrices.\n",
    "# Learning:        Expectation maximization of each model's parameters against images of their corresponding face IDs.\n",
    "# Inference:       Computation of the posterior over face IDs, given an input face image vector.\n",
    "\n",
    "# Requirements:\n",
    "#             -> [st] training_data[]    - Array with rows of grayscale face images \n",
    "#             -> [st] training_labels[]  - Vector of the faces' corresponding IDs\n",
    "#             -> [fn] dnorm()            - Multivariate normal density function\n",
    "#             -> [fn] posterior()        - Receives image vector, returns class probabilities\n",
    "#             -> [st] priors[]           - Class priors indexed by world state\n",
    "#             -> [fn] fit_priors()       - Receives training set, returns vector of class priors\n",
    "#             -> [fn] likelihood()       - Receives image vector + world state, returns probability\n",
    "#             -> [fn] fit_likelihoods()  - Iterates over world states and fits their corresponding likelihood functions\n",
    "#             -> [fn] EM()               - Maximizes likelihood for fit_likelihoods()\n",
    "#             -> [fn] E_step()           - Computes hidden variable posteriors for EM()\n",
    "#             -> [fn] M_step()           - Maximizes boundary w.r.t. likelihood fn parameters for EM()\n",
    "#             -> [fn] boundary()         - Computes boundary value for EM()\n",
    "#             -> [st] likelihood_params[]- List containing parameters corresponding to kth likelihood function\n",
    "#             -> [st] test_data[]        - Vector of test image grayscale pixel intensities\n",
    "\n",
    "#` 1. Import and flatten the data.\n",
    "#` 2. Fit the prior distribution     : prior = fit_prior(vector of class labels)\n",
    "#`    1.1. Compute and return the relative class sizes.\n",
    "#` 3. Fit the likelihood distribution: likelihood_parameters = fit_likelihood(matrix of image vectors, vector of class labels, n_factors)\n",
    "#`   3.1. Iterate over each class and maximize its images' likelihoods - Pr(x|w,th) - wr.t. th via EM.\n",
    "#        -> Call: [mu_k, phi_k, covar_k] = EM(matrix of class image vectors, n_factors)\n",
    "#       3.1.1. Randomly initialize vector mu_k, matrix phi_k, diag. matrix covar_k.\n",
    "#       3.1.2. E_step(): Maximize the boundary w.r.t. the density functions over the hidden variable.\n",
    "#       3.1.3. M_step(): Maximize the boundary's value w.r.t. mu_k, phi_k, and covar_k.\n",
    "#       3.1.4. boundary(): Compute the boundary's value given the parameters and density functions.\n",
    "#`    3.2. Store [mu_k, phi_k, covar_k] in an array such that they are indexed by world state.\n",
    "#`    3.3. Return the array of parameter values.\n",
    "#  4. Inference: [vector of class probabilities] = posterior(input_vec, parameter_arr)\n",
    "#    4.1. Compute the product likelihood(input_vec, parameter_arr, k)*prior[k] for all world states k.\n",
    "#       4.1.1. likelihood() is a multivariate normal density calculation.\n",
    "#    4.2. Normalize and return this vector of unnormalized probabilities.\n",
    "# 5. Display the image along with the computed class probabilities.\n",
    "\n",
    "# What do the factors represent? Directions in which covariance is greatest among pixels.\n",
    "# *Extracting the factors and plotting them as images would be interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pytest\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import and label the training data\n",
    "face_ids        = [fp[-6:] for fp in glob.iglob('.\\\\data\\\\face-identification\\\\select-faces\\\\*')]\n",
    "face_fps        = glob.glob('.\\\\data\\\\face-identification\\\\select-faces\\\\*\\\\*.jpg')\n",
    "training_data   = np.array([np.ravel(imresize(imread(fp, flatten=True), size = [60, 60])) for fp in face_fps])\n",
    "training_labels = [face_ids.index(name) for name in [fp.split('\\\\')[-2] for fp in face_fps]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test functions for fit_likelihood()\n",
    "def test_likelihood_inputs(training_data, training_labels, n_factors):\n",
    "    assert n_factors > 0\n",
    "    assert type(training_labels)    == list\n",
    "    assert type(training_data)      == np.ndarray\n",
    "    assert type(training_labels[0]) == int\n",
    "    assert len(training_labels)     == training_data.shape[0]\n",
    "\n",
    "def test_likelihood_outputs(klikelihood_params, datum_length, n_factors):\n",
    "    assert test_mean_dim(klikelihood_params[1], datum_length)\n",
    "    assert test_factor_dim(klikelihood_params[2], datum_length, n_factors)\n",
    "    assert test_covar_dim(klikelihood_params[3], datum_length)\n",
    "    assert test_covar_psd(klikelihood_params[3])\n",
    "    assert test_covar_sym(klikelihood_params[3])\n",
    "\n",
    "def test_likelihood_mean_dim(mean_vector, datum_length):\n",
    "    assert len(mean_vector) == datum_length\n",
    "\n",
    "def test_likelihood_factor_dim(factor_matrix, datum_length, n_factors):\n",
    "    assert factor_matrix.shape == (datum_length, n_factors)\n",
    "\n",
    "def test_likelihood_covar_positive(covar_matrix):\n",
    "    assert np.all(covar_matrix >= 0)\n",
    "    \n",
    "def test_likelihood_covar_dim(covar_matrix, datum_length):\n",
    "    assert covar_matrix.shape == (datum_length, datum_length)\n",
    "    \n",
    "def test_likelihood_covar_sym(covar_matrix):\n",
    "    assert np.all(covar_matrix == covar_matrix.T)\n",
    "\n",
    "def test_likelihood_covar_psd(covar_matrix):\n",
    "    assert np.all(np.linalg.eigvals(covar_matrix) >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_prior(training_labels):\n",
    "    # Receives a list of training labels indicating face ID\n",
    "    # Returns a numpy array of ID prior probabilities\n",
    "    priors = [sum(training_labels == ID)/len(training_labels) for ID in np.unique(training_labels)]\n",
    "    return np.array(priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_likelihood(training_data, training_labels, n_factors):\n",
    "    # training_data   -> numpy array with rows of grayscale image vectors\n",
    "    # training_labels -> list of associated integer face IDs\n",
    "    # n_factors       -> Number of factors to use in factor analyzer\n",
    "    # Iterates over world states and fits the parameters of their associated likelihood distributions\n",
    "    test_inputs(training_data, training_labels, n_factors)\n",
    "    class_IDs         = np.unique(training_labels)\n",
    "    likelihood_params = [None]*len(classes) # List containing a set of parameters for each world state\n",
    "    datum_length      = training_data.shape[1]\n",
    "    for k in np.unique(class_IDs):\n",
    "        k_data               = training_data[training_labels == k, :] # Images containing kth person\n",
    "        klikelihood_params   = EM(k_data, n_factors, datum_length) # Fit the kth FA's parameters -> [mu_k, phi_k, covar_k]\n",
    "        assert test_likelihood_outputs(klikelihood_params, datum_length, n_factors)\n",
    "        likelihood_params[k] = klikelihood_params\n",
    "    return likelihood_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EM(training_data, n_factors, datum_length):\n",
    "    # Returns parameters that fit a factor analyzer to the given data\n",
    "    # training_data -> np array with rows of vectorized input images\n",
    "    # n_factors     -> Number of factors to model the data's covariance with\n",
    "    # 1. Randomly initialize parameters\n",
    "    mu      = np.random.uniform([255]*datum_length) # Vector\n",
    "    phi     = np.array([np.random.normal(size = datum_length) for _ in range(n_factors)]) # Matrix\n",
    "    covar   = np.random.normal()*np.identity(datum_length)\n",
    "    n_datum = training_data.shape[0]\n",
    "    # 2. Iterate over E and M steps until boundary ceases to shift\n",
    "    while abs(upd_bound - bound) > 1e-4\n",
    "        [mu, phi, covar]             = [mu_upd, phi_upd, covar_upd]\n",
    "        expectations                 = E_step(training_data, mu, phi, covar, n_datum) # [N x 2] np.array: [(E[h_i], E[h_i h_i.T]), ...]\n",
    "        [mu_upd, phi_upd, covar_upd] = M_step(training_data, expectations, n_datum) #\n",
    "        error                        = np.max(abs(np.array([mu_upd - mu], [phi_upd - phi], [covar_upd - covar]))\n",
    "    return [mu_upd, phi_upd, covar_upd]\n",
    "# The hidden variable is a vector describing the shift in the normal's mean through the subspace - this shift is conditional\n",
    "# on each image, meaning that well-represented images will produce many shifts to a particular part of the subspace. This\n",
    "# further means that the factors will tend to be aligned in the direction of shift (see the computation of the factors in\n",
    "# the M-step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def E_step(training_data, mu, phi, covar, n_datum):\n",
    "    mu = np.mat(mu).T; phi = np.mat(phi); covar = np.mat(covar); training_data = np.mat(training_data);\n",
    "    I  = np.identity(len(mu))\n",
    "    expectations = [None]*n_datum\n",
    "    for i in range(n_datum):\n",
    "        x                = np.mat(training_data[i, :]).T\n",
    "        inv_term         = np.linalg.inv(phi.T * np.linalg.inv(covar) * phi + I)\n",
    "        E_h              = inv_term * phi.T * np.linalg.inv(covar) * (x - mu)\n",
    "        E_hh             = inv_term + E_h * E_h.T\n",
    "        expectations[i]  = [E_h, E_hh]\n",
    "    return expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def M_step(training_data, expectations, n_datum):\n",
    "    x         = training_data\n",
    "    # Mu\n",
    "    mu_hat    = np.sum(training_data, axis = 0)\n",
    "    # Phi\n",
    "    list_E_hh = np.array([expec[2] for expec in expectations])\n",
    "    list_E_h  = np.array([expec[1] for expec in expectations])\n",
    "    t1_list   = [x[i, :].T * list_E_h[i].T for i in range(n_datum)] # List of matrices\n",
    "    phi_t1    = np.sum(t1_list, axis = 2)\n",
    "    phi_t2    = np.linalg.inv(np.sum(t2_list, axis = 2))\n",
    "    phi_hat   = phi_t1 * phi_t2\n",
    "    # Sigma\n",
    "    for i in range(n_datum):\n",
    "        sum_of_sq += np.diag((x[i, :].T - mu_hat) * (x[i, :].T - mu_hat).T - phi_hat * list_E_h[i] * (x[i, :].T - mu_hat).T)\n",
    "    covar     = sum_of_sq/n_datum\n",
    "    return [mu_hat, phi_hat, covar_hat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_factors = 1\n",
    "fit_parameters = fit_likelihood(training_data, training_labels, n_factors)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
